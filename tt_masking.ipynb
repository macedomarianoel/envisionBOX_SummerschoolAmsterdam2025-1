{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rg8gwF2_W34"
      },
      "source": [
        "# SECTION 1: INSTALLATION AND SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cq4MkLOo_U2F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üü¢ Google Colab was not detected.\n",
            "Create a virtual environment and select that as your kernel.\n",
            "An example is shown in comments below.\n",
            "You may be asked to install ipykernel in your virtual environment.\n"
          ]
        }
      ],
      "source": [
        "# MediaPipe Body Tracking - Educational Tutorial\n",
        "# A step-by-step guide to full-body tracking, masking, and privacy protection\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if we're in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"üîµ Running in Google Colab\")\n",
        "except ImportError: # assuming you have conda installed\n",
        "    IN_COLAB = False\n",
        "    print(\"üü¢ Google Colab was not detected.\\n\" \\\n",
        "           \"Create a virtual environment and select that as your kernel.\\n\" \\\n",
        "             \"An example is shown in comments below.\") \n",
        "    print(\"You may be asked to install ipykernel in your virtual environment.\")\n",
        "    # conda create -n tt_masking -y python=3.11.13\n",
        "    # conda activate tt_masking   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üì¶ STEP 1: Installing required packages...\n",
            "Requirement already satisfied: mediapipe in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (0.10.21)\n",
            "Requirement already satisfied: opencv-python-headless in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (4.11.0.86)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (3.10.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (2.3.0)\n",
            "Requirement already satisfied: ipywidgets in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (8.1.7)\n",
            "Requirement already satisfied: tqdm in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: absl-py in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (0.6.2)\n",
            "Requirement already satisfied: jaxlib in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (0.6.2)\n",
            "Requirement already satisfied: numpy<2 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipywidgets) (9.3.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
            "Requirement already satisfied: colorama in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: decorator in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
            "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
            "Requirement already satisfied: stack_data in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
            "Requirement already satisfied: wcwidth in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: pycparser in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from jax->mediapipe) (0.5.1)\n",
            "Requirement already satisfied: opt_einsum in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.12 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from jax->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in c:\\users\\kings\\miniconda3\\envs\\tt_masking\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüì¶ STEP 1: Installing required packages...\")\n",
        "!pip install mediapipe opencv-python-headless matplotlib pandas ipywidgets tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "X6vY4oe-_fMm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All packages imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import io\n",
        "import base64\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from tqdm.auto import tqdm\n",
        "from IPython.display import display, Video, HTML, clear_output\n",
        "\n",
        "print(\"‚úÖ All packages imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUVnwC72_hoy"
      },
      "source": [
        "# SECTION 2: UNDERSTANDING MEDIAPIPE ANATOMY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hQuDbuZV_p5u"
      },
      "outputs": [],
      "source": [
        "class MediaPipeAnatomy:\n",
        "    \"\"\"\n",
        "    A class to help students understand MediaPipe's tracking capabilities\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Body landmarks (33 points)\n",
        "        self.body_landmarks = [\n",
        "            'NOSE', 'LEFT_EYE_INNER', 'LEFT_EYE', 'LEFT_EYE_OUTER',\n",
        "            'RIGHT_EYE_INNER', 'RIGHT_EYE', 'RIGHT_EYE_OUTER',\n",
        "            'LEFT_EAR', 'RIGHT_EAR', 'MOUTH_LEFT', 'MOUTH_RIGHT',\n",
        "            'LEFT_SHOULDER', 'RIGHT_SHOULDER', 'LEFT_ELBOW', 'RIGHT_ELBOW',\n",
        "            'LEFT_WRIST', 'RIGHT_WRIST', 'LEFT_PINKY', 'RIGHT_PINKY',\n",
        "            'LEFT_INDEX', 'RIGHT_INDEX', 'LEFT_THUMB', 'RIGHT_THUMB',\n",
        "            'LEFT_HIP', 'RIGHT_HIP', 'LEFT_KNEE', 'RIGHT_KNEE',\n",
        "            'LEFT_ANKLE', 'RIGHT_ANKLE', 'LEFT_HEEL', 'RIGHT_HEEL',\n",
        "            'LEFT_FOOT_INDEX', 'RIGHT_FOOT_INDEX'\n",
        "        ]\n",
        "\n",
        "        # Hand landmarks (21 points per hand)\n",
        "        hand_parts = ['WRIST', 'THUMB_CMC', 'THUMB_MCP', 'THUMB_IP', 'THUMB_TIP',\n",
        "                     'INDEX_FINGER_MCP', 'INDEX_FINGER_PIP', 'INDEX_FINGER_DIP', 'INDEX_FINGER_TIP',\n",
        "                     'MIDDLE_FINGER_MCP', 'MIDDLE_FINGER_PIP', 'MIDDLE_FINGER_DIP', 'MIDDLE_FINGER_TIP',\n",
        "                     'RING_FINGER_MCP', 'RING_FINGER_PIP', 'RING_FINGER_DIP', 'RING_FINGER_TIP',\n",
        "                     'PINKY_FINGER_MCP', 'PINKY_FINGER_PIP', 'PINKY_FINGER_DIP', 'PINKY_FINGER_TIP']\n",
        "\n",
        "        self.hand_landmarks = []\n",
        "        for side in ['LEFT', 'RIGHT']:\n",
        "            for part in hand_parts:\n",
        "                self.hand_landmarks.append(f\"{side}_{part}\") # ex: wrist is added as left_wrist and right_wrist\n",
        "\n",
        "        # Face landmarks (478 points)\n",
        "        self.face_landmarks = [str(i) for i in range(478)]\n",
        "\n",
        "    def show_summary(self):\n",
        "        \"\"\"Display a summary of tracking capabilities\"\"\"\n",
        "        print(\"üéØ MediaPipe Tracking Capabilities:\")\n",
        "        print(f\"   üë§ Body: {len(self.body_landmarks)} keypoints\")\n",
        "        print(f\"   ‚úã Hands: {len(self.hand_landmarks)} keypoints (21 per hand)\")\n",
        "        print(f\"   üòä Face: {len(self.face_landmarks)} keypoints\")\n",
        "        print(f\"   üìä Total: {len(self.body_landmarks) + len(self.hand_landmarks) + len(self.face_landmarks)} tracking points!\")\n",
        "\n",
        "        return {\n",
        "            'body': len(self.body_landmarks),\n",
        "            'hands': len(self.hand_landmarks),\n",
        "            'face': len(self.face_landmarks)\n",
        "        }\n",
        "\n",
        "    def create_column_names(self):\n",
        "        \"\"\"Create column names for CSV export\"\"\"\n",
        "        body_columns = ['time']\n",
        "        hand_columns = ['time']\n",
        "        face_columns = ['time']\n",
        "\n",
        "        # Body columns (X, Y, Z, visibility)\n",
        "        for landmark in self.body_landmarks:\n",
        "            for coord in ['X', 'Y', 'Z', 'visibility']:\n",
        "                body_columns.append(f\"{coord}_{landmark}\")\n",
        "\n",
        "        # Hand columns (X, Y, Z)\n",
        "        for landmark in self.hand_landmarks:\n",
        "            for coord in ['X', 'Y', 'Z']:\n",
        "                hand_columns.append(f\"{coord}_{landmark}\")\n",
        "\n",
        "        # Face columns (X, Y, Z)\n",
        "        for landmark in self.face_landmarks:\n",
        "            for coord in ['X', 'Y', 'Z']:\n",
        "                face_columns.append(f\"{coord}_{landmark}\")\n",
        "\n",
        "        return body_columns, hand_columns, face_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß† STEP 2: Understanding what MediaPipe can track...\n",
            "üéØ MediaPipe Tracking Capabilities:\n",
            "   üë§ Body: 33 keypoints\n",
            "   ‚úã Hands: 42 keypoints (21 per hand)\n",
            "   üòä Face: 478 keypoints\n",
            "   üìä Total: 553 tracking points!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüß† STEP 2: Understanding what MediaPipe can track...\")\n",
        "anatomy = MediaPipeAnatomy()\n",
        "tracking_info = anatomy.show_summary()\n",
        "body_cols, hand_cols, face_cols = anatomy.create_column_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kma2RHv-_q4N"
      },
      "source": [
        "# SECTION 3: CONFIGURATION\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Pl4bry2G_vXo"
      },
      "outputs": [],
      "source": [
        "class MediaPipeConfig:\n",
        "    \"\"\"\n",
        "    Interactive configuration for MediaPipe processing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.setup_widgets()\n",
        "\n",
        "    def setup_widgets(self):\n",
        "        \"\"\"Create interactive widgets for configuration\"\"\"\n",
        "\n",
        "        # Privacy options\n",
        "        self.skeleton_widget = widgets.Checkbox(value=True, description='Show skeleton overlay')\n",
        "        self.face_only_widget = widgets.Checkbox(value=False, description='Face skeleton only')\n",
        "        self.white_bg_widget = widgets.Checkbox(value=False, description='White background mode')\n",
        "\n",
        "        # Masking options\n",
        "        self.mask_body_widget = widgets.Checkbox(value=False, description='Mask body (black silhouette)')\n",
        "        self.mask_face_widget = widgets.Checkbox(value=False, description='Mask face (black)')\n",
        "\n",
        "        # Blurring options\n",
        "        self.blur_body_widget = widgets.Checkbox(value=True, description='Blur body')\n",
        "        self.blur_face_widget = widgets.Checkbox(value=False, description='Blur face')\n",
        "        self.blur_intensity_widget = widgets.FloatSlider(\n",
        "            value=1.0, min=0.0, max=1.0, step=0.1,\n",
        "            description='Blur intensity:', style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "        # Trace options\n",
        "        self.finger_traces_widget = widgets.Checkbox(value=True, description='Add finger traces')\n",
        "        self.trace_length_widget = widgets.FloatSlider(\n",
        "            value=2.0, min=0.5, max=5.0, step=0.5,\n",
        "            description='Trace length (seconds):', style={'description_width': 'initial'}\n",
        "        )\n",
        "\n",
        "    def show_interface(self):\n",
        "        \"\"\"Display the configuration interface\"\"\"\n",
        "\n",
        "        privacy_box = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üîí Privacy & Visualization</h3>\"),\n",
        "            self.skeleton_widget,\n",
        "            self.face_only_widget,\n",
        "            self.white_bg_widget\n",
        "        ])\n",
        "\n",
        "        masking_box = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üé≠ Masking Options</h3>\"),\n",
        "            self.mask_body_widget,\n",
        "            self.mask_face_widget\n",
        "        ])\n",
        "\n",
        "        blurring_box = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üå´Ô∏è Blurring Options</h3>\"),\n",
        "            self.blur_body_widget,\n",
        "            self.blur_face_widget,\n",
        "            self.blur_intensity_widget\n",
        "        ])\n",
        "\n",
        "        tracing_box = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>‚úèÔ∏è Movement Tracing</h3>\"),\n",
        "            self.finger_traces_widget,\n",
        "            self.trace_length_widget\n",
        "        ])\n",
        "\n",
        "        config_interface = widgets.HBox([privacy_box, masking_box, blurring_box, tracing_box])\n",
        "\n",
        "        return config_interface\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"Get current configuration as dictionary\"\"\"\n",
        "        return {\n",
        "            'skeleton': self.skeleton_widget.value,\n",
        "            'skeleton_face_only': self.face_only_widget.value,\n",
        "            'whitebackground': self.white_bg_widget.value,\n",
        "            'maskingbody': self.mask_body_widget.value,\n",
        "            'maskingface': self.mask_face_widget.value,\n",
        "            'blurringbody': self.blur_body_widget.value,\n",
        "            'blurringface': self.blur_face_widget.value,\n",
        "            'blurringfactor': self.blur_intensity_widget.value,\n",
        "            'add_finger_traces': self.finger_traces_widget.value,\n",
        "            'trace_length_seconds': self.trace_length_widget.value,\n",
        "            'trace_color_left': (0, 255, 0),  # Green\n",
        "            'trace_color_right': (0, 0, 255)  # Blue\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚öôÔ∏è STEP 3: Interactive configuration panel...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n‚öôÔ∏è STEP 3: Interactive configuration panel...\")\n",
        "config_panel = MediaPipeConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roLkZn2y_wwN"
      },
      "source": [
        "# SECTION 4: HELPER FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Fgdpkmp0_0I_"
      },
      "outputs": [],
      "source": [
        "def analyze_csv_output(video_name):\n",
        "    \"\"\"\n",
        "    Analyze and visualize the CSV output data\n",
        "\n",
        "    Args:\n",
        "        video_name (str): Name of the processed video (without extension)\n",
        "    \"\"\"\n",
        "    print(f\"üìä Analyzing CSV data for: {video_name}\")\n",
        "\n",
        "    try:\n",
        "        # Load CSV files\n",
        "        body_df = pd.read_csv(f'Output_TimeSeries/{video_name}_body.csv')\n",
        "        hand_df = pd.read_csv(f'Output_TimeSeries/{video_name}_hands.csv')\n",
        "        face_df = pd.read_csv(f'Output_TimeSeries/{video_name}_face.csv')\n",
        "\n",
        "        print(f\"‚úÖ Loaded data files:\")\n",
        "        print(f\"   üìä Body: {len(body_df)} frames, {len(body_df.columns)} columns\")\n",
        "        print(f\"   ‚úã Hand: {len(hand_df)} frames, {len(hand_df.columns)} columns\")\n",
        "        print(f\"   üòä Face: {len(face_df)} frames, {len(face_df.columns)} columns\")\n",
        "\n",
        "        # Create visualizations\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle(f'MediaPipe Analysis: {video_name}', fontsize=16)\n",
        "\n",
        "        # Plot 1: Body landmark visibility over time\n",
        "        axes[0, 0].plot(body_df['time'], body_df['visibility_NOSE'], label='Nose', alpha=0.7)\n",
        "        axes[0, 0].plot(body_df['time'], body_df['visibility_LEFT_WRIST'], label='Left Wrist', alpha=0.7)\n",
        "        axes[0, 0].plot(body_df['time'], body_df['visibility_RIGHT_WRIST'], label='Right Wrist', alpha=0.7)\n",
        "        axes[0, 0].set_title('Body Landmark Visibility')\n",
        "        axes[0, 0].set_xlabel('Time (ms)')\n",
        "        axes[0, 0].set_ylabel('Visibility Score')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Hand movement (if available)\n",
        "        if 'X_LEFT_INDEX_FINGER_TIP' in hand_df.columns:\n",
        "            # Calculate hand velocities\n",
        "            left_x = hand_df['X_LEFT_INDEX_FINGER_TIP'].dropna()\n",
        "            left_y = hand_df['Y_LEFT_INDEX_FINGER_TIP'].dropna()\n",
        "\n",
        "            if len(left_x) > 1:\n",
        "                left_velocity = np.sqrt(np.diff(left_x)**2 + np.diff(left_y)**2)\n",
        "                axes[0, 1].plot(left_velocity, label='Left Index Finger', alpha=0.7)\n",
        "                axes[0, 1].set_title('Hand Movement Velocity')\n",
        "                axes[0, 1].set_xlabel('Frame')\n",
        "                axes[0, 1].set_ylabel('Velocity (normalized units)')\n",
        "                axes[0, 1].legend()\n",
        "                axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 3: Body pose trajectory (nose movement)\n",
        "        axes[1, 0].plot(body_df['X_NOSE'], body_df['Y_NOSE'], 'o-', alpha=0.6, markersize=2)\n",
        "        axes[1, 0].set_title('Head Movement Trajectory')\n",
        "        axes[1, 0].set_xlabel('X Position (normalized)')\n",
        "        axes[1, 0].set_ylabel('Y Position (normalized)')\n",
        "        axes[1, 0].invert_yaxis()  # Invert Y axis to match image coordinates\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 4: Data completeness heatmap\n",
        "        completeness_data = []\n",
        "        for col in ['Body', 'Hands', 'Face']:\n",
        "            if col == 'Body':\n",
        "                df_subset = body_df.select_dtypes(include=[np.number])\n",
        "            elif col == 'Hands':\n",
        "                df_subset = hand_df.select_dtypes(include=[np.number])\n",
        "            else:\n",
        "                df_subset = face_df.select_dtypes(include=[np.number])\n",
        "\n",
        "            completeness = (1 - df_subset.isnull().sum() / len(df_subset)) * 100\n",
        "            completeness_data.append(completeness.mean())\n",
        "\n",
        "        axes[1, 1].bar(['Body', 'Hands', 'Face'], completeness_data,\n",
        "                      color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
        "        axes[1, 1].set_title('Data Completeness')\n",
        "        axes[1, 1].set_ylabel('Completeness (%)')\n",
        "        axes[1, 1].set_ylim(0, 100)\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Add percentage labels on bars\n",
        "        for i, v in enumerate(completeness_data):\n",
        "            axes[1, 1].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print summary statistics\n",
        "        print(f\"\\nüìà Summary Statistics:\")\n",
        "        print(f\"   ‚è±Ô∏è Duration: {body_df['time'].max()/1000:.1f} seconds\")\n",
        "        print(f\"   üìä Data completeness:\")\n",
        "        print(f\"      üë§ Body: {completeness_data[0]:.1f}%\")\n",
        "        print(f\"      ‚úã Hands: {completeness_data[1]:.1f}%\")\n",
        "        print(f\"      üòä Face: {completeness_data[2]:.1f}%\")\n",
        "\n",
        "        return {\n",
        "            'body_df': body_df,\n",
        "            'hand_df': hand_df,\n",
        "            'face_df': face_df,\n",
        "            'completeness': completeness_data\n",
        "        }\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"‚ùå Could not find CSV files for {video_name}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "def upload_sample_video():\n",
        "    \"\"\"\n",
        "    Helper function to upload sample video in Colab\n",
        "    \"\"\"\n",
        "    if IN_COLAB:\n",
        "        from google.colab import files\n",
        "        print(\"üì§ Upload a video file for testing:\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        for filename in uploaded.keys():\n",
        "            # Move uploaded file to Input_Videos folder\n",
        "            import shutil\n",
        "            shutil.move(filename, f'Input_Videos/{filename}')\n",
        "            print(f\"‚úÖ Uploaded {filename} to Input_Videos/\")\n",
        "\n",
        "            # Show video info\n",
        "            show_video_info(f'Input_Videos/{filename}')\n",
        "\n",
        "        return list(uploaded.keys())\n",
        "    else:\n",
        "        print(\"üìÅ Please manually add video files to the Input_Videos/ folder\")\n",
        "        return []\n",
        "\n",
        "def get_video_list(video_dir: str):\n",
        "    if not os.path.exists(video_dir):\n",
        "        raise FileNotFoundError(f\"Video directory {video_dir} does not exist\")\n",
        "    \n",
        "    videos = [f for f in os.listdir(video_dir)\n",
        "               if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "    return videos\n",
        "\n",
        "def parse_mediapipe_landmarks(landmarks):\n",
        "    \"\"\"\n",
        "    Convert MediaPipe landmarks to clean numerical values\n",
        "\n",
        "    Args:\n",
        "        landmarks: MediaPipe landmark object\n",
        "\n",
        "    Returns:\n",
        "        list: Clean numerical coordinates\n",
        "    \"\"\"\n",
        "    if landmarks is None:\n",
        "        return []\n",
        "\n",
        "    coordinates = []\n",
        "    for landmark in landmarks.landmark:\n",
        "        coordinates.extend([str(landmark.x), str(landmark.y), str(landmark.z)])\n",
        "\n",
        "    return coordinates\n",
        "\n",
        "def create_folders():\n",
        "    \"\"\"Create necessary folders for input/output\"\"\"\n",
        "    folders = ['Input_Videos', 'Output_Videos', 'Output_TimeSeries']\n",
        "\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            os.makedirs(folder)\n",
        "            print(f\"üìÅ Created folder: {folder}\")\n",
        "        else:\n",
        "            print(f\"‚úÖ Folder exists: {folder}\")\n",
        "\n",
        "    return folders\n",
        "\n",
        "def get_video_specs(video_path):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        raise f\"‚ùå Could not open video: {video_path}\"\n",
        "        \n",
        "    # Get video properties\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    duration = frame_count / fps if fps > 0 else 0\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    return fps, frame_count, width, height, duration\n",
        "\n",
        "def show_video_info(video_path):\n",
        "    \"\"\"Display information about the input video\"\"\"\n",
        "\n",
        "    fps, frame_count, width, height, duration = get_video_specs(video_path)\n",
        "\n",
        "    info = {\n",
        "        'fps': fps,\n",
        "        'frame_count': frame_count,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'duration': duration\n",
        "    }\n",
        "\n",
        "    print(f\"üìπ Video Information:\")\n",
        "    print(f\"   üìè Dimensions: {width} x {height}\")\n",
        "    print(f\"   üé¨ Frame rate: {fps:.1f} FPS\")\n",
        "    print(f\"   ‚è±Ô∏è Duration: {duration:.1f} seconds\")\n",
        "    print(f\"   üéûÔ∏è Total frames: {frame_count}\")\n",
        "\n",
        "    return info\n",
        "\n",
        "def display_frame_with_landmarks(image, results, config):\n",
        "    \"\"\"\n",
        "    Display a single frame with MediaPipe landmarks for demonstration\n",
        "\n",
        "    Args:\n",
        "        image: Input image\n",
        "        results: MediaPipe results\n",
        "        config: Configuration dictionary\n",
        "    \"\"\"\n",
        "    # Initialize MediaPipe drawing utilities\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "    mp_drawing_styles = mp.solutions.drawing_styles\n",
        "    mp_holistic = mp.solutions.holistic\n",
        "\n",
        "    # Convert image for display\n",
        "    display_image = image.copy()\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        if config['skeleton']:\n",
        "            if not config['skeleton_face_only']:\n",
        "                # Draw pose landmarks\n",
        "                mp_drawing.draw_landmarks(\n",
        "                display_image,\n",
        "                results.pose_landmarks,\n",
        "                mp_holistic.POSE_CONNECTIONS,\n",
        "                landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style()\n",
        "                )\n",
        "                # Draw hand landmarks\n",
        "                mp_drawing.draw_landmarks(\n",
        "                display_image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "                mp_drawing.draw_landmarks(\n",
        "                display_image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
        "\n",
        "            # Draw face landmarks\n",
        "            mp_drawing.draw_landmarks(\n",
        "                display_image,\n",
        "                results.face_landmarks,\n",
        "                mp_holistic.FACEMESH_TESSELATION,\n",
        "                landmark_drawing_spec=None,\n",
        "                connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
        "            )\n",
        "\n",
        "    return display_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üîß STEP 4: Creating Folders\n",
            "‚úÖ Folder exists: Input_Videos\n",
            "‚úÖ Folder exists: Output_Videos\n",
            "‚úÖ Folder exists: Output_TimeSeries\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß STEP 4: Creating Folders\")\n",
        "created_folders = create_folders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x_Fz0xS_1gb"
      },
      "source": [
        "# SECTION 5: MAIN PROCESSING CLASS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DgTySo2f_8NN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ STEP 5: Understand the Main processing engine...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüöÄ STEP 5: Understand the Main processing engine...\")\n",
        "\n",
        "class MediaPipeProcessor:\n",
        "    \"\"\"\n",
        "    Main class for processing videos with MediaPipe\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.mp_holistic = mp.solutions.holistic\n",
        "        self.mp_drawing = mp.solutions.drawing_utils\n",
        "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "    def process_single_video(self, video_path, output_video_path=None, save_csv=True):\n",
        "        \"\"\"\n",
        "        Process a single video with MediaPipe\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to input video\n",
        "            output_video_path (str): Path for output video (optional)\n",
        "            save_csv (bool): Whether to save CSV data\n",
        "\n",
        "        Returns:\n",
        "            dict: Processing results and data\n",
        "        \"\"\"\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        fps, total_frames, width, height, _ = get_video_specs(video_path)\n",
        "\n",
        "        # Setup video writer if output path provided\n",
        "        if output_video_path:\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "            out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "        # Initialize data storage\n",
        "        body_data = [body_cols]\n",
        "        hand_data = [hand_cols]\n",
        "        face_data = [face_cols]\n",
        "\n",
        "        # Initialize finger trace buffers\n",
        "        if self.config['add_finger_traces']:\n",
        "            trace_frames = int(self.config['trace_length_seconds'] * fps)\n",
        "            left_finger_trace = []\n",
        "            right_finger_trace = []\n",
        "\n",
        "        # Process video frame by frame\n",
        "        frame_count = 0\n",
        "        time_ms = 0\n",
        "\n",
        "        with self.mp_holistic.Holistic(\n",
        "            static_image_mode=False,\n",
        "            enable_segmentation=True,\n",
        "            refine_face_landmarks=True\n",
        "        ) as holistic:\n",
        "\n",
        "            # Progress bar for long videos\n",
        "            pbar = tqdm(total=total_frames, desc=\"Processing frames\")\n",
        "\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Convert BGR to RGB for MediaPipe\n",
        "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "                # Process frame with MediaPipe\n",
        "                results = holistic.process(rgb_frame)\n",
        "\n",
        "                # Create output frame\n",
        "                output_frame = self._apply_effects(frame, results)\n",
        "\n",
        "                # Add finger traces if enabled\n",
        "                if self.config['add_finger_traces']:\n",
        "                    output_frame = self._add_finger_traces(\n",
        "                        output_frame, results,\n",
        "                        left_finger_trace, right_finger_trace,\n",
        "                        trace_frames, width, height\n",
        "                    )\n",
        "\n",
        "                # Draw skeleton if enabled\n",
        "                if self.config['skeleton']:\n",
        "                    output_frame = self._draw_skeleton(output_frame, results)\n",
        "\n",
        "                # Save frame to output video\n",
        "                if output_video_path:\n",
        "                    out.write(output_frame)\n",
        "\n",
        "                # Extract and save landmark data\n",
        "                self._extract_landmark_data(\n",
        "                    results, time_ms, body_data, hand_data, face_data\n",
        "                )\n",
        "\n",
        "                # Update progress\n",
        "                frame_count += 1\n",
        "                time_ms += (1000 / fps)\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Show progress every 30 frames\n",
        "                if frame_count % 30 == 0:\n",
        "                    pbar.set_postfix({\n",
        "                        'Time': f\"{time_ms/1000:.1f}s\",\n",
        "                        'Pose': '‚úÖ' if results.pose_landmarks else '‚ùå'\n",
        "                    })\n",
        "\n",
        "        # Cleanup\n",
        "        pbar.close()\n",
        "        cap.release()\n",
        "        if output_video_path:\n",
        "            out.release()\n",
        "\n",
        "        # Save CSV data if requested\n",
        "        if save_csv:\n",
        "            video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "            self._save_csv_data(video_name, body_data, hand_data, face_data)\n",
        "\n",
        "        return {\n",
        "            'frames_processed': frame_count,\n",
        "            'duration': time_ms / 1000,\n",
        "            'body_data': body_data,\n",
        "            'hand_data': hand_data,\n",
        "            'face_data': face_data\n",
        "        }\n",
        "\n",
        "    def _apply_effects(self, frame, results):\n",
        "        \"\"\"Apply masking and blurring effects\"\"\"\n",
        "        h, w, c = frame.shape\n",
        "        output_frame = frame.copy()\n",
        "\n",
        "        # White background mode\n",
        "        if self.config['whitebackground']:\n",
        "            output_frame = np.full((h, w, 3), 255, dtype=np.uint8)\n",
        "            return output_frame\n",
        "\n",
        "        # Body blurring\n",
        "        if self.config['blurringbody'] and results.segmentation_mask is not None:\n",
        "            # Create body mask\n",
        "            mask = (results.segmentation_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "            # Apply Gaussian blur\n",
        "            kernel_size = int(51 * self.config['blurringfactor'])\n",
        "            if kernel_size % 2 == 0:\n",
        "                kernel_size += 1\n",
        "\n",
        "            blurred = cv2.GaussianBlur(output_frame, (kernel_size, kernel_size), 0)\n",
        "\n",
        "            # Blend blurred and original\n",
        "            mask_3d = np.stack([mask] * 3, axis=-1)\n",
        "            output_frame = np.where(mask_3d, blurred, output_frame)\n",
        "\n",
        "        # Body masking\n",
        "        if self.config['maskingbody'] and results.segmentation_mask is not None:\n",
        "            mask = (results.segmentation_mask > 0.5).astype(np.uint8)\n",
        "            mask_3d = np.stack([mask] * 3, axis=-1)\n",
        "            output_frame = np.where(mask_3d, 0, output_frame)\n",
        "\n",
        "        # Face effects\n",
        "        if results.face_landmarks and (self.config['blurringface'] or self.config['maskingface']):\n",
        "            # Create face mask\n",
        "            face_points = []\n",
        "            for landmark in results.face_landmarks.landmark:\n",
        "                x = int(landmark.x * w)\n",
        "                y = int(landmark.y * h)\n",
        "                face_points.append([x, y])\n",
        "\n",
        "            face_points = np.array(face_points, dtype=np.int32)\n",
        "            face_mask = np.zeros((h, w), dtype=np.uint8)\n",
        "            cv2.fillConvexPoly(face_mask, cv2.convexHull(face_points), 255)\n",
        "\n",
        "            if self.config['blurringface']:\n",
        "                kernel_size = int(51 * self.config['blurringfactor'])\n",
        "                if kernel_size % 2 == 0:\n",
        "                    kernel_size += 1\n",
        "                blurred = cv2.GaussianBlur(output_frame, (kernel_size, kernel_size), 0)\n",
        "                face_mask_3d = np.stack([face_mask] * 3, axis=-1) / 255.0\n",
        "                output_frame = (output_frame * (1 - face_mask_3d) + blurred * face_mask_3d).astype(np.uint8)\n",
        "\n",
        "            if self.config['maskingface']:\n",
        "                output_frame[face_mask > 0] = 0\n",
        "\n",
        "        return output_frame\n",
        "\n",
        "    def _add_finger_traces(self, frame, results, left_trace, right_trace, max_frames, width, height):\n",
        "        \"\"\"Add finger movement traces\"\"\"\n",
        "        # Get current finger positions\n",
        "        if results.left_hand_landmarks:\n",
        "            landmark = results.left_hand_landmarks.landmark[8]  # Index finger tip\n",
        "            pos = (int(landmark.x * width), int(landmark.y * height))\n",
        "            left_trace.append(pos)\n",
        "\n",
        "        if results.right_hand_landmarks:\n",
        "            landmark = results.right_hand_landmarks.landmark[8]  # Index finger tip\n",
        "            pos = (int(landmark.x * width), int(landmark.y * height))\n",
        "            right_trace.append(pos)\n",
        "\n",
        "        # Limit trace length\n",
        "        if len(left_trace) > max_frames:\n",
        "            left_trace.pop(0)\n",
        "        if len(right_trace) > max_frames:\n",
        "            right_trace.pop(0)\n",
        "\n",
        "        # Draw traces\n",
        "        for i, trace in enumerate([left_trace, right_trace]):\n",
        "            color = self.config['trace_color_left'] if i == 0 else self.config['trace_color_right']\n",
        "\n",
        "            for j in range(len(trace) - 1):\n",
        "                if j + 1 < len(trace):\n",
        "                    alpha = (j + 1) / len(trace)\n",
        "                    overlay = frame.copy()\n",
        "                    cv2.line(overlay, trace[j], trace[j + 1], color, 2)\n",
        "                    frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def _draw_skeleton(self, frame, results):\n",
        "        \"\"\"Draw skeleton landmarks\"\"\"\n",
        "        if self.config['skeleton_face_only']:\n",
        "            # Draw only face\n",
        "            if results.face_landmarks:\n",
        "                self.mp_drawing.draw_landmarks(\n",
        "                    frame, results.face_landmarks,\n",
        "                    self.mp_holistic.FACEMESH_TESSELATION,\n",
        "                    landmark_drawing_spec=None,\n",
        "                    connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
        "                )\n",
        "        else:\n",
        "            # Draw full skeleton\n",
        "            if results.pose_landmarks:\n",
        "                self.mp_drawing.draw_landmarks(\n",
        "                    frame, results.pose_landmarks,\n",
        "                    self.mp_holistic.POSE_CONNECTIONS,\n",
        "                    landmark_drawing_spec=self.mp_drawing_styles.get_default_pose_landmarks_style()\n",
        "                )\n",
        "\n",
        "            if results.left_hand_landmarks:\n",
        "                self.mp_drawing.draw_landmarks(\n",
        "                    frame, results.left_hand_landmarks,\n",
        "                    self.mp_holistic.HAND_CONNECTIONS\n",
        "                )\n",
        "\n",
        "            if results.right_hand_landmarks:\n",
        "                self.mp_drawing.draw_landmarks(\n",
        "                    frame, results.right_hand_landmarks,\n",
        "                    self.mp_holistic.HAND_CONNECTIONS\n",
        "                )\n",
        "\n",
        "            if results.face_landmarks:\n",
        "                self.mp_drawing.draw_landmarks(\n",
        "                    frame, results.face_landmarks,\n",
        "                    self.mp_holistic.FACEMESH_TESSELATION,\n",
        "                    landmark_drawing_spec=None,\n",
        "                    connection_drawing_spec=self.mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
        "                )\n",
        "\n",
        "        return frame\n",
        "\n",
        "    def _extract_landmark_data(self, results, time_ms, body_data, hand_data, face_data):\n",
        "        \"\"\"Extract landmark coordinates for CSV export\"\"\"\n",
        "        # Body data\n",
        "        if results.pose_landmarks:\n",
        "            body_row = [time_ms]\n",
        "            for landmark in results.pose_landmarks.landmark:\n",
        "                body_row.extend([landmark.x, landmark.y, landmark.z, landmark.visibility])\n",
        "        else:\n",
        "            body_row = [time_ms] + [np.nan] * (len(body_cols) - 1)\n",
        "        body_data.append(body_row)\n",
        "\n",
        "        # Hand data\n",
        "        hand_row = [time_ms]\n",
        "\n",
        "        # Left hand\n",
        "        if results.left_hand_landmarks:\n",
        "            for landmark in results.left_hand_landmarks.landmark:\n",
        "                hand_row.extend([landmark.x, landmark.y, landmark.z])\n",
        "        else:\n",
        "            hand_row.extend([np.nan] * 63)  # 21 landmarks * 3 coordinates\n",
        "\n",
        "        # Right hand\n",
        "        if results.right_hand_landmarks:\n",
        "            for landmark in results.right_hand_landmarks.landmark:\n",
        "                hand_row.extend([landmark.x, landmark.y, landmark.z])\n",
        "        else:\n",
        "            hand_row.extend([np.nan] * 63)\n",
        "\n",
        "        hand_data.append(hand_row)\n",
        "\n",
        "        # Face data\n",
        "        if results.face_landmarks:\n",
        "            face_row = [time_ms]\n",
        "            for landmark in results.face_landmarks.landmark:\n",
        "                face_row.extend([landmark.x, landmark.y, landmark.z])\n",
        "        else:\n",
        "            face_row = [time_ms] + [np.nan] * (len(face_cols) - 1)\n",
        "        face_data.append(face_row)\n",
        "\n",
        "    def _save_csv_data(self, video_name, body_data, hand_data, face_data):\n",
        "        \"\"\"Save extracted data to CSV files\"\"\"\n",
        "        # Save body data\n",
        "        body_df = pd.DataFrame(body_data[1:], columns=body_data[0])\n",
        "        body_df.to_csv(f'Output_TimeSeries/{video_name}_body.csv', index=False)\n",
        "\n",
        "        # Save hand data\n",
        "        hand_df = pd.DataFrame(hand_data[1:], columns=hand_data[0])\n",
        "        hand_df.to_csv(f'Output_TimeSeries/{video_name}_hands.csv', index=False)\n",
        "\n",
        "        # Save face data\n",
        "        face_df = pd.DataFrame(face_data[1:], columns=face_data[0])\n",
        "        face_df.to_csv(f'Output_TimeSeries/{video_name}_face.csv', index=False)\n",
        "\n",
        "        print(f\"üíæ Saved CSV files for {video_name}\")\n",
        "        print(f\"   üìä Body data: {len(body_df)} frames, {len(body_df.columns)} columns\")\n",
        "        print(f\"   ‚úã Hand data: {len(hand_df)} frames, {len(hand_df.columns)} columns\")\n",
        "        print(f\"   üòä Face data: {len(face_df)} frames, {len(face_df.columns)} columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YYzRcay_9wy"
      },
      "source": [
        "# SECTION 6: DEMONSTRATION AND TESTING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demo_processing():\n",
        "    \"\"\"\n",
        "    Demonstration function that can be called after setup\n",
        "    \"\"\"\n",
        "    print(\"üéØ Demo: Processing videos with current configuration...\")\n",
        "\n",
        "    # Get current configuration\n",
        "    current_config = config_panel.get_config()\n",
        "\n",
        "    # Show configuration summary\n",
        "    print(\"\\nüìã Current Configuration:\")\n",
        "    for key, value in current_config.items():\n",
        "        if isinstance(value, bool):\n",
        "            status = \"‚úÖ\" if value else \"‚ùå\"\n",
        "            print(f\"   {status} {key}: {value}\")\n",
        "        else:\n",
        "            print(f\"   üîß {key}: {value}\")\n",
        "\n",
        "    # Check for videos in input folder\n",
        "    input_videos = get_video_list('Input_Videos')\n",
        "\n",
        "    if not input_videos:\n",
        "        print(\"\\n‚ö†Ô∏è No videos found in Input_Videos folder!\")\n",
        "        print(\"Please add a video file to Input_Videos/ folder and try again.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nüìπ Found {len(input_videos)} video(s) to process:\")\n",
        "    for video in input_videos:\n",
        "        print(f\"   üé¨ {video}\")\n",
        "\n",
        "    # Process each video\n",
        "    processor = MediaPipeProcessor(current_config)\n",
        "\n",
        "    for video in input_videos:\n",
        "        print(f\"\\nüöÄ Processing: {video}\")\n",
        "\n",
        "        input_path = f\"Input_Videos/{video}\"\n",
        "        output_path = f\"Output_Videos/{video}\"\n",
        "\n",
        "        # Show video info\n",
        "        video_info = show_video_info(input_path)\n",
        "\n",
        "        if video_info is None:\n",
        "            continue\n",
        "        try:\n",
        "            # Process the video\n",
        "            results = processor.process_single_video(\n",
        "                input_path, output_path, save_csv=True\n",
        "            )\n",
        "\n",
        "            print(f\"‚úÖ Successfully processed {results['frames_processed']} frames\")\n",
        "            print(f\"‚è±Ô∏è Video duration: {results['duration']:.1f} seconds\")\n",
        "            print(f\"üìÅ Output saved to: {output_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing {video}: {str(e)}\")\n",
        "\n",
        "    print(\"\\nüéâ Processing complete! Check your Output_Videos and Output_TimeSeries folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All setup complete! Ready to process videos.\n",
            "1. Configure your processing options below:\n",
            "2. Add video files to the 'Input_Videos' folder\n",
            "3. Run demo_processing() to start processing\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f03d1b2d78f44fe85b0177c3eeb8b12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(VBox(children=(HTML(value='<h3>üîí Privacy & Visualization</h3>'), Checkbox(value=True, descripti‚Ä¶"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"‚úÖ All setup complete! Ready to process videos.\")\n",
        "print(\"1. Configure your processing options below:\")\n",
        "print(\"2. Add video files to the 'Input_Videos' folder\")\n",
        "print(\"3. Run demo_processing() to start processing\")\n",
        "config_panel.show_interface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üéØ Demo: Processing videos with current configuration...\n",
            "\n",
            "üìã Current Configuration:\n",
            "   ‚ùå skeleton: False\n",
            "   ‚ùå skeleton_face_only: False\n",
            "   ‚ùå whitebackground: False\n",
            "   ‚ùå maskingbody: False\n",
            "   ‚ùå maskingface: False\n",
            "   ‚úÖ blurringbody: True\n",
            "   ‚ùå blurringface: False\n",
            "   üîß blurringfactor: 1.0\n",
            "   ‚ùå add_finger_traces: False\n",
            "   üîß trace_length_seconds: 1.9999999999999998\n",
            "   üîß trace_color_left: (0, 255, 0)\n",
            "   üîß trace_color_right: (0, 0, 255)\n",
            "\n",
            "üìπ Found 1 video(s) to process:\n",
            "   üé¨ video2.mp4\n",
            "\n",
            "üöÄ Processing: video2.mp4\n",
            "üìπ Video Information:\n",
            "   üìè Dimensions: 1280 x 720\n",
            "   üé¨ Frame rate: 50.0 FPS\n",
            "   ‚è±Ô∏è Duration: 21.4 seconds\n",
            "   üéûÔ∏è Total frames: 1069\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecf05c3c4ef6472998ad0db86fe74827",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing frames:   0%|          | 0/1069 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ Saved CSV files for video2\n",
            "   üìä Body data: 1069 frames, 133 columns\n",
            "   ‚úã Hand data: 1069 frames, 127 columns\n",
            "   üòä Face data: 1069 frames, 1435 columns\n",
            "‚úÖ Successfully processed 1069 frames\n",
            "‚è±Ô∏è Video duration: 21.4 seconds\n",
            "üìÅ Output saved to: Output_Videos/video2.mp4\n",
            "\n",
            "üéâ Processing complete! Check your Output_Videos and Output_TimeSeries folders.\n"
          ]
        }
      ],
      "source": [
        "demo_processing()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uUudwPKANn-"
      },
      "source": [
        "# SECTION 7: EDUCATIONAL EXERCISES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_learning_exercises():\n",
        "    \"\"\"\n",
        "    Create structured learning exercises for students\n",
        "    \"\"\"\n",
        "    exercises = {\n",
        "        \"Exercise 1: Basic Understanding\": {\n",
        "            \"description\": \"Explore MediaPipe's tracking capabilities\",\n",
        "            \"tasks\": [\n",
        "                \"1. Count the total number of trackable points\",\n",
        "                \"2. Identify which body parts have visibility scores\",\n",
        "                \"3. Explain the coordinate system (X, Y, Z ranges)\"\n",
        "            ],\n",
        "            \"code\": \"\"\"\n",
        "# Your code here to explore the anatomy object\n",
        "print(\"Body landmarks:\", len(anatomy.body_landmarks))\n",
        "print(\"Hand landmarks:\", len(anatomy.hand_landmarks))\n",
        "print(\"Face landmarks:\", len(anatomy.face_landmarks))\n",
        "\n",
        "# Try accessing specific landmark names\n",
        "print(\"First 5 body landmarks:\", anatomy.body_landmarks[:5])\n",
        "            \"\"\"\n",
        "        },\n",
        "\n",
        "        \"Exercise 2: Configuration Exploration\": {\n",
        "            \"description\": \"Experiment with different privacy settings\",\n",
        "            \"tasks\": [\n",
        "                \"1. Try 3 different blurring intensities\",\n",
        "                \"2. Compare masking vs blurring effects\",\n",
        "                \"3. Test skeleton-only mode\"\n",
        "            ],\n",
        "            \"code\": \"\"\"\n",
        "# Your code here to test different configurations\n",
        "config1 = {'blurringbody': True, 'blurringfactor': 0.3}\n",
        "config2 = {'blurringbody': True, 'blurringfactor': 0.7}\n",
        "config3 = {'maskingbody': True, 'skeleton': True}\n",
        "\n",
        "# Process the same video with each config and compare\n",
        "            \"\"\"\n",
        "        },\n",
        "\n",
        "        \"Exercise 3: Data Analysis\": {\n",
        "            \"description\": \"Analyze movement patterns from CSV output\",\n",
        "            \"tasks\": [\n",
        "                \"1. Calculate average hand movement velocity\",\n",
        "                \"2. Find frames with missing pose data\",\n",
        "                \"3. Plot head movement trajectory\"\n",
        "            ],\n",
        "            \"code\": \"\"\"\n",
        "# Your code here for data analysis\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your processed data\n",
        "# body_df = pd.read_csv('Output_TimeSeries/your_video_body.csv')\n",
        "\n",
        "# Calculate velocities, find missing data, create plots\n",
        "            \"\"\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìö Learning Exercises Available:\n",
            "\n",
            "Exercise 1: Basic Understanding:\n",
            "  üìù Explore MediaPipe's tracking capabilities\n",
            "     1. Count the total number of trackable points\n",
            "     2. Identify which body parts have visibility scores\n",
            "     3. Explain the coordinate system (X, Y, Z ranges)\n",
            "\n",
            "Exercise 2: Configuration Exploration:\n",
            "  üìù Experiment with different privacy settings\n",
            "     1. Try 3 different blurring intensities\n",
            "     2. Compare masking vs blurring effects\n",
            "     3. Test skeleton-only mode\n",
            "\n",
            "Exercise 3: Data Analysis:\n",
            "  üìù Analyze movement patterns from CSV output\n",
            "     1. Calculate average hand movement velocity\n",
            "     2. Find frames with missing pose data\n",
            "     3. Plot head movement trajectory\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüìö Learning Exercises Available:\")\n",
        "exercises = create_learning_exercises()\n",
        "for title, content in exercises.items():\n",
        "    print(f\"\\n{title}:\")\n",
        "    print(f\"  üìù {content['description']}\")\n",
        "    for task in content['tasks']:\n",
        "        print(f\"     {task}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SECTION 8: INTERACTIVE DASHBOARD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_demo():\n",
        "    \"\"\"\n",
        "    Create a side-by-side comparison of different processing options\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Creating comparison demo...\")\n",
        "\n",
        "    # Define different configurations to compare\n",
        "    configs = {\n",
        "        'Original': {\n",
        "            'skeleton': False, 'skeleton_face_only': False, 'whitebackground': False,\n",
        "            'maskingbody': False, 'maskingface': False, 'blurringbody': False,\n",
        "            'blurringface': False, 'blurringfactor': 1.0, 'add_finger_traces': False,\n",
        "            'trace_length_seconds': 2.0, 'trace_color_left': (0, 255, 0), 'trace_color_right': (0, 0, 255)\n",
        "        },\n",
        "        'Skeleton Only': {\n",
        "            'skeleton': True, 'skeleton_face_only': False, 'whitebackground': True,\n",
        "            'maskingbody': False, 'maskingface': False, 'blurringbody': False,\n",
        "            'blurringface': False, 'blurringfactor': 1.0, 'add_finger_traces': False,\n",
        "            'trace_length_seconds': 2.0, 'trace_color_left': (0, 255, 0), 'trace_color_right': (0, 0, 255)\n",
        "        },\n",
        "        'Body Blur + Skeleton': {\n",
        "            'skeleton': True, 'skeleton_face_only': False, 'whitebackground': False,\n",
        "            'maskingbody': False, 'maskingface': False, 'blurringbody': True,\n",
        "            'blurringface': False, 'blurringfactor': 1.0, 'add_finger_traces': True,\n",
        "            'trace_length_seconds': 2.0, 'trace_color_left': (0, 255, 0), 'trace_color_right': (0, 0, 255)\n",
        "        },\n",
        "        'Face Privacy + Traces': {\n",
        "            'skeleton': True, 'skeleton_face_only': False, 'whitebackground': False,\n",
        "            'maskingbody': False, 'maskingface': True, 'blurringbody': False,\n",
        "            'blurringface': False, 'blurringfactor': 1.0, 'add_finger_traces': True,\n",
        "            'trace_length_seconds': 2.0, 'trace_color_left': (0, 255, 0), 'trace_color_right': (0, 0, 255)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Check for input videos\n",
        "    input_videos = [f for f in os.listdir('Input_Videos')\n",
        "                   if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "\n",
        "    if not input_videos:\n",
        "        print(\"‚ùå No videos found for comparison demo in Input_Videos Folder\")\n",
        "        return\n",
        "\n",
        "    video_file = input_videos[0]  # Use first available video\n",
        "    print(f\"üé¨ Using video: {video_file}\")\n",
        "\n",
        "    # Process with each configuration\n",
        "    comparison_results = {}\n",
        "\n",
        "    for config_name, config in configs.items():\n",
        "        print(f\"\\nüîÑ Processing with {config_name} settings...\")\n",
        "\n",
        "        processor = MediaPipeProcessor(config)\n",
        "        output_path = f\"Output_Videos/{config_name}_{video_file}\"\n",
        "\n",
        "        try:\n",
        "            results = processor.process_single_video(\n",
        "                f\"Input_Videos/{video_file}\",\n",
        "                output_path,\n",
        "                save_csv=False\n",
        "            )\n",
        "            comparison_results[config_name] = {\n",
        "                'success': True,\n",
        "                'output_path': output_path,\n",
        "                'frames': results['frames_processed']\n",
        "            }\n",
        "            print(f\"‚úÖ {config_name}: {results['frames_processed']} frames processed\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå {config_name}: Error - {str(e)}\")\n",
        "            comparison_results[config_name] = {\n",
        "                'success': False,\n",
        "                'error': str(e)\n",
        "            }\n",
        "\n",
        "    print(f\"\\nüéâ Comparison demo complete!\")\n",
        "    print(f\"üìÅ Check Output_Videos/ folder for results\")\n",
        "\n",
        "    return comparison_results\n",
        "\n",
        "def create_comparison_grid(video_pattern=None, output_name=\"comparison\", max_duration=10, fps=10):\n",
        "    \"\"\"\n",
        "    Create a side-by-side comparison video/GIF from processed videos\n",
        "\n",
        "    Args:\n",
        "        video_pattern (str): Pattern to match videos (e.g., \"ted_kid\") or None for all\n",
        "        output_name (str): Name for output files\n",
        "        max_duration (float): Maximum duration in seconds\n",
        "        fps (int): Frame rate for output\n",
        "    \"\"\"\n",
        "    print(\"üé¨ Creating comparison video/GIF...\")\n",
        "\n",
        "    # Find all processed videos\n",
        "    output_videos = get_video_list('Output_Videos')\n",
        "\n",
        "    if video_pattern:\n",
        "        output_videos = [f for f in output_videos if video_pattern.lower() in f.lower()]\n",
        "\n",
        "    if len(output_videos) < 2:\n",
        "        print(f\"‚ùå Need at least 2 videos for comparison. Found: {len(output_videos)}\")\n",
        "        print(f\"   Available videos: {output_videos}\")\n",
        "        return\n",
        "\n",
        "    print(f\"üìπ Found {len(output_videos)} videos for comparison:\")\n",
        "    for i, video in enumerate(output_videos, 1):\n",
        "        print(f\"   {i}. {video}\")\n",
        "\n",
        "    # Sort videos for consistent ordering\n",
        "    output_videos.sort()\n",
        "\n",
        "    # Read all videos and get properties\n",
        "    video_caps = []\n",
        "    min_frames = float('inf')\n",
        "    target_width, target_height = 320, 240  # Smaller size for grid\n",
        "\n",
        "    for video_file in output_videos:\n",
        "        cap = cv2.VideoCapture(f'Output_Videos/{video_file}')\n",
        "        \n",
        "        if cap.isOpened():\n",
        "            video_fps, frame_count, _, _, _ = get_video_specs(f'Output_Videos/{video_file}')\n",
        "            max_frames_allowed = int(max_duration * video_fps)\n",
        "            actual_frames = min(frame_count, max_frames_allowed)\n",
        "            min_frames = min(min_frames, actual_frames)\n",
        "            video_caps.append((cap, video_file))\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Could not open {video_file}\")\n",
        "\n",
        "    if not video_caps:\n",
        "        print(\"‚ùå Could not open any videos\")\n",
        "        return\n",
        "\n",
        "    # Calculate grid layout\n",
        "    n_videos = len(video_caps)\n",
        "    grid_cols = int(np.ceil(np.sqrt(n_videos)))\n",
        "    grid_rows = int(np.ceil(n_videos / grid_cols))\n",
        "\n",
        "    grid_width = target_width * grid_cols\n",
        "    grid_height = target_height * grid_rows\n",
        "\n",
        "    print(f\"üìê Creating {grid_rows}x{grid_cols} grid ({grid_width}x{grid_height})\")\n",
        "    print(f\"‚è±Ô∏è Processing {min_frames} frames at {fps} FPS\")\n",
        "\n",
        "    # Setup video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    comparison_video = cv2.VideoWriter(\n",
        "        f'Output_Videos/{output_name}_comparison.mp4',\n",
        "        fourcc, fps, (grid_width, grid_height)\n",
        "    )\n",
        "\n",
        "    # Store frames for GIF creation\n",
        "    gif_frames = []\n",
        "\n",
        "    # Process frames\n",
        "    pbar = tqdm(total=min_frames, desc=\"Creating comparison\")\n",
        "\n",
        "    for frame_idx in range(min_frames):\n",
        "        # Create empty grid\n",
        "        grid_frame = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)\n",
        "\n",
        "        # Read frame from each video\n",
        "        for i, (cap, video_file) in enumerate(video_caps):\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # Resize frame to fit grid\n",
        "                resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "\n",
        "                # Calculate position in grid\n",
        "                row = i // grid_cols\n",
        "                col = i % grid_cols\n",
        "\n",
        "                y_start = row * target_height\n",
        "                y_end = y_start + target_height\n",
        "                x_start = col * target_width\n",
        "                x_end = x_start + target_width\n",
        "\n",
        "                # Place frame in grid\n",
        "                grid_frame[y_start:y_end, x_start:x_end] = resized_frame\n",
        "\n",
        "                # Add label\n",
        "                label = video_file.replace('.mp4', '').replace('_', ' ')\n",
        "                if len(label) > 20:\n",
        "                    label = label[:17] + \"...\"\n",
        "\n",
        "                cv2.putText(grid_frame, label,\n",
        "                           (x_start + 5, y_start + 20),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
        "\n",
        "        # Save frame to video\n",
        "        comparison_video.write(grid_frame)\n",
        "\n",
        "        # Save every nth frame for GIF (to reduce size)\n",
        "        if frame_idx % max(1, min_frames // 50) == 0:  # Max 50 frames for GIF\n",
        "            # Convert BGR to RGB for GIF\n",
        "            rgb_frame = cv2.cvtColor(grid_frame, cv2.COLOR_BGR2RGB)\n",
        "            gif_frames.append(rgb_frame)\n",
        "\n",
        "        pbar.update(1)\n",
        "\n",
        "    # Cleanup\n",
        "    pbar.close()\n",
        "    comparison_video.release()\n",
        "    for cap, _ in video_caps:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"‚úÖ Comparison video saved: Output_Videos/{output_name}_comparison.mp4\")\n",
        "\n",
        "    # Create GIF\n",
        "    try:\n",
        "        from PIL import Image\n",
        "        print(\"üéûÔ∏è Creating GIF...\")\n",
        "\n",
        "        gif_images = [Image.fromarray(frame) for frame in gif_frames]\n",
        "\n",
        "        gif_path = f'Output_Videos/{output_name}_comparison.gif'\n",
        "        gif_images[0].save(\n",
        "            gif_path,\n",
        "            save_all=True,\n",
        "            append_images=gif_images[1:],\n",
        "            duration=int(1000/fps * max(1, min_frames // len(gif_frames))),\n",
        "            loop=0\n",
        "        )\n",
        "        print(f\"‚úÖ GIF saved: {gif_path}\")\n",
        "\n",
        "        # Display GIF info\n",
        "        gif_size = os.path.getsize(gif_path) / 1024 / 1024  # MB\n",
        "        print(f\"üìä GIF info: {len(gif_frames)} frames, {gif_size:.1f} MB\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è PIL not available - GIF creation skipped\")\n",
        "        print(\"   Install with: pip install Pillow\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è GIF creation failed: {str(e)}\")\n",
        "\n",
        "    return f'Output_Videos/{output_name}_comparison.mp4'\n",
        "\n",
        "def create_before_after_comparison(original_video, processed_videos=None):\n",
        "    \"\"\"\n",
        "    Create a before/after comparison with the original video\n",
        "\n",
        "    Args:\n",
        "        original_video (str): Path to original video in Input_Videos/\n",
        "        processed_videos (list): List of processed video names, or None for all\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Creating before/after comparison...\")\n",
        "\n",
        "    if not os.path.exists(f'Input_Videos/{original_video}'):\n",
        "        print(f\"‚ùå Original video not found: Input_Videos/{original_video}\")\n",
        "        return\n",
        "\n",
        "    # Find processed versions\n",
        "    video_name = os.path.splitext(original_video)[0]\n",
        "    if processed_videos is None:\n",
        "        processed_videos = [f for f in os.listdir('Output_Videos')\n",
        "                          if video_name.lower() in f.lower() and f.endswith('.mp4')]\n",
        "\n",
        "    if not processed_videos:\n",
        "        print(f\"‚ùå No processed videos found for {video_name}\")\n",
        "        return\n",
        "\n",
        "    # Add original video to comparison\n",
        "    all_videos = [f'../Input_Videos/{original_video}'] + [f'Output_Videos/{v}' for v in processed_videos]\n",
        "    labels = ['Original'] + [v.replace('.mp4', '').replace(f'{video_name}_', '') for v in processed_videos]\n",
        "\n",
        "    print(f\"üìπ Comparing: {len(all_videos)} versions\")\n",
        "    for i, (video, label) in enumerate(zip(all_videos, labels)):\n",
        "        print(f\"   {i+1}. {label}\")\n",
        "\n",
        "    # Create side-by-side comparison\n",
        "    caps = []\n",
        "    for video_path in all_videos:\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        if cap.isOpened():\n",
        "            caps.append(cap)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Could not open {video_path}\")\n",
        "\n",
        "    if len(caps) < 2:\n",
        "        print(\"‚ùå Need at least 2 videos for comparison\")\n",
        "        return\n",
        "\n",
        "    # Get video properties\n",
        "    fps = caps[0].get(cv2.CAP_PROP_FPS)\n",
        "    frame_count = min([int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) for cap in caps])\n",
        "\n",
        "    # Create grid layout\n",
        "    n_videos = len(caps)\n",
        "    cols = min(3, n_videos)  # Max 3 columns\n",
        "    rows = int(np.ceil(n_videos / cols))\n",
        "\n",
        "    frame_width = 300\n",
        "    frame_height = 200\n",
        "    grid_width = frame_width * cols\n",
        "    grid_height = frame_height * rows\n",
        "\n",
        "    # Setup output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    output_path = f'Output_Videos/{video_name}_before_after.mp4'\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (grid_width, grid_height))\n",
        "\n",
        "    print(f\"üé¨ Creating {rows}x{cols} comparison video...\")\n",
        "\n",
        "    pbar = tqdm(total=min(frame_count, int(10 * fps)), desc=\"Processing frames\")\n",
        "\n",
        "    for frame_idx in range(min(frame_count, int(10 * fps))):  # Max 10 seconds\n",
        "        grid_frame = np.zeros((grid_height, grid_width, 3), dtype=np.uint8)\n",
        "\n",
        "        for i, (cap, label) in enumerate(zip(caps, labels)):\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                # Resize and place in grid\n",
        "                resized = cv2.resize(frame, (frame_width, frame_height))\n",
        "\n",
        "                row = i // cols\n",
        "                col = i % cols\n",
        "                y_start = row * frame_height\n",
        "                x_start = col * frame_width\n",
        "\n",
        "                grid_frame[y_start:y_start+frame_height, x_start:x_start+frame_width] = resized\n",
        "\n",
        "                # Add label\n",
        "                cv2.putText(grid_frame, label,\n",
        "                           (x_start + 10, y_start + 30),\n",
        "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "        out.write(grid_frame)\n",
        "        pbar.update(1)\n",
        "\n",
        "    # Cleanup\n",
        "    pbar.close()\n",
        "    out.release()\n",
        "    for cap in caps:\n",
        "        cap.release()\n",
        "\n",
        "    print(f\"‚úÖ Before/after comparison saved: {output_path}\")\n",
        "    return output_path\n",
        "\n",
        "def auto_create_comparisons():\n",
        "    \"\"\"\n",
        "    Automatically create comparison videos for all processed content\n",
        "    \"\"\"\n",
        "    print(\"üéØ Auto-creating all comparison videos...\")\n",
        "\n",
        "    # Find unique video names\n",
        "    input_videos = get_video_list('Input_Videos')\n",
        "    output_videos = get_video_list('Output_Videos')\n",
        "\n",
        "    if not output_videos:\n",
        "        print(\"‚ùå No processed videos found in Output_Videos/\")\n",
        "        return\n",
        "\n",
        "    # Create overall comparison of all processed videos\n",
        "    print(\"\\n1Ô∏è‚É£ Creating overall comparison grid...\")\n",
        "    create_comparison_grid(output_name=\"all_effects\")\n",
        "\n",
        "    # Create before/after for each original video\n",
        "    for original_video in input_videos:\n",
        "        video_name = os.path.splitext(original_video)[0]\n",
        "        matching_processed = [f for f in output_videos if video_name.lower() in f.lower()]\n",
        "\n",
        "        if matching_processed:\n",
        "            print(f\"\\n2Ô∏è‚É£ Creating before/after for {original_video}...\")\n",
        "            create_before_after_comparison(original_video)\n",
        "\n",
        "    print(\"\\nüéâ All comparison videos created!\")\n",
        "    print(\"üìÅ Check Output_Videos/ folder for:\")\n",
        "    print(\"   üé¨ all_effects_comparison.mp4 - Grid of all effects\")\n",
        "    print(\"   üéûÔ∏è all_effects_comparison.gif - Animated GIF\")\n",
        "    print(\"   üîÑ *_before_after.mp4 - Before/after comparisons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import uuid\n",
        "class CreateProcessingDashboard():\n",
        "    def __init__(self):\n",
        "        self.output_area = widgets.Output()\n",
        "        self.video_selector, self.process_button, self.analyze_button, self.upload_button, self.before_after_button, self.all_possible_outputs_button, self.compare_processed_button, self.all_input_output_button = self.create_buttons() \n",
        "        \n",
        "        self.file_section = widgets.VBox([\n",
        "        widgets.HTML(\"<h3>üìÅ File Management</h3>\"),\n",
        "        self.video_selector,\n",
        "        widgets.HBox([self.upload_button, self.process_button])\n",
        "        ])\n",
        "\n",
        "        self.analysis_section = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üìä Analysis Tools</h3>\"),\n",
        "            widgets.HBox([self.analyze_button])\n",
        "        ])\n",
        "\n",
        "        self.grid_section = widgets.VBox([\n",
        "            widgets.HTML(\"<h3>üìä Compare different Videos in a Grid</h3>\"),\n",
        "            widgets.HBox([self.before_after_button, self.all_possible_outputs_button, self.compare_processed_button, self.all_input_output_button])\n",
        "        ])\n",
        "\n",
        "        self.dashboard = widgets.VBox([\n",
        "            widgets.HTML(\"<h2>üéõÔ∏è MediaPipe Processing Dashboard</h2>\"),\n",
        "            self.file_section,\n",
        "            self.analysis_section,\n",
        "            self.grid_section,\n",
        "            self.output_area\n",
        "        ])\n",
        "\n",
        "    def upload_handler(self, b):\n",
        "        unique_id = uuid.uuid4()\n",
        "\n",
        "        self.output_area.clear_output()\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            print(\"Upload button clicked\", unique_id)\n",
        "            print(f\"id(self.output_area): {id(self.output_area)}\")\n",
        "            print(f\"widget_repr: {repr(self.output_area)}\")\n",
        "\n",
        "    def update_video_list(self):\n",
        "        videos = get_video_list('Input_Videos')\n",
        "        return videos if videos else ['No videos found in Input_Videos Folder']\n",
        "\n",
        "    def create_custom_button(self, description, button_style, on_click_handler):\n",
        "        button = widgets.Button(\n",
        "            description=description,\n",
        "            button_style=button_style,\n",
        "            layout=widgets.Layout(width='200px')\n",
        "        )\n",
        "        if not hasattr(button, '_handler_attached'):\n",
        "            button.on_click(on_click_handler)\n",
        "            button._handler_attached = True\n",
        "\n",
        "        return button\n",
        "\n",
        "    def create_buttons(self):\n",
        "        video_selector = widgets.Dropdown(\n",
        "        options= self.update_video_list(),\n",
        "        description='Select Video:',\n",
        "        style={'description_width': 'initial'},\n",
        "        )\n",
        "\n",
        "        process_button = self.create_custom_button(description='üöÄ Process Video', button_style=\"success\", on_click_handler=self.upload_handler)\n",
        "        analyze_button = self.create_custom_button(description='üìä Analyze Data', button_style=\"info\", on_click_handler=self.on_analyze_click)\n",
        "        upload_button = self.create_custom_button(description='üì§ Upload Video', button_style=\"primary\", on_click_handler=self.on_upload_click)\n",
        "        before_after_button = self.create_custom_button(description='Before/After', button_style=\"primary\", on_click_handler=self.create_before_after_comparison_click)\n",
        "        all_possible_outputs_button = self.create_custom_button(description='All Processsing Options', button_style=\"primary\", on_click_handler=self.create_comparison_demo_click)\n",
        "        compare_processed_button = self.create_custom_button(description='All Processed', button_style=\"primary\", on_click_handler=self.create_comparison_grid_click)\n",
        "        all_input_output_button = self.create_custom_button(description='All Input and Processed', button_style=\"primary\", on_click_handler=self.auto_create_comparisons_click)\n",
        "\n",
        "        return video_selector, process_button, analyze_button, upload_button, before_after_button, all_possible_outputs_button, compare_processed_button, all_input_output_button\n",
        "    \n",
        "    def on_process_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            selected_video = self.video_selector.value\n",
        "            if selected_video and selected_video != 'No videos found in Input_Videos Folder':\n",
        "                current_config = config_panel.get_config()\n",
        "                processor = MediaPipeProcessor(current_config)\n",
        "\n",
        "                input_path = f\"Input_Videos/{selected_video}\"\n",
        "                output_path = f\"Output_Videos/processed_{selected_video}\"\n",
        "\n",
        "                print(f\"üöÄ Processing {selected_video}...\")\n",
        "                try:\n",
        "                    results = processor.process_single_video(input_path, output_path, save_csv=True)\n",
        "                    print(f\"‚úÖ Success! Processed {results['frames_processed']} frames\")\n",
        "\n",
        "                    # Update video list in case new files were added\n",
        "                    self.video_selector.options = self.update_video_list()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error: {str(e)}\")\n",
        "            else:\n",
        "                print(\"‚ùå Please select a valid video file\")\n",
        "\n",
        "    def create_before_after_comparison_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            selected_video = self.video_selector.value\n",
        "            if selected_video and selected_video != 'No videos found in Input_Videos Folder':\n",
        "                create_before_after_comparison(selected_video)\n",
        "            else:\n",
        "                print(\"‚ùå Please select a valid video file you want to compare\")\n",
        "\n",
        "    def create_comparison_demo_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            create_comparison_demo()\n",
        "           \n",
        "    def create_comparison_grid_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            create_comparison_grid()\n",
        "            \n",
        "    def auto_create_comparisons_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            auto_create_comparisons()\n",
        "            \n",
        "    def on_analyze_click(self, b): # b is not used but is required by the library\n",
        "        self.output_area.clear_output()\n",
        "        with self.output_area:\n",
        "            selected_video = self.video_selector.value\n",
        "            if selected_video and selected_video != 'No videos found in Input_Videos Folder':\n",
        "                video_name = os.path.splitext(selected_video)[0]\n",
        "                analyze_csv_output(video_name)\n",
        "            else:\n",
        "                print(\"‚ùå Please select a valid video file\")\n",
        "\n",
        "    def on_upload_click(self, b): # b is not used but is required by the library\n",
        "        with self.output_area:\n",
        "            clear_output(wait=True)\n",
        "            uploaded_files = upload_sample_video()\n",
        "            if uploaded_files:\n",
        "                # Update video selector with new files\n",
        "                self.video_selector.options = self.update_video_list()\n",
        "                print(f\"‚úÖ Upload complete! Select from dropdown above.\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "dashboard = CreateProcessingDashboard()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf0442df0bfa4dfaadff6ddd28b5cb83",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "HBox(children=(VBox(children=(HTML(value='<h3>üîí Privacy & Visualization</h3>'), Checkbox(value=True, descripti‚Ä¶"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config_panel.show_interface()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ac9aad7598541b1bbbc2156311edacd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<h2>üéõÔ∏è MediaPipe Processing Dashboard</h2>'), VBox(children=(HTML(value='<h3>üìÅ File‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(dashboard.dashboard) # why is it processing multiple times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tt_masking",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
